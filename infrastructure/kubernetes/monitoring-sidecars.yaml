# Monitoring Sidecars Configuration for Bolt Applications
# Comprehensive observability sidecars for production monitoring

apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-config
  namespace: bolt-logging
data:
  fluentd.conf: |
    # Fluentd configuration for Bolt logging aggregation
    <source>
      @type tail
      @id bolt_app_logs
      path /app/logs/*.log
      pos_file /tmp/fluentd-bolt-app.pos
      tag bolt.app
      
      <parse>
        @type json
        time_key timestamp
        time_format %Y-%m-%dT%H:%M:%S.%LZ
        keep_time_key true
      </parse>
      
      # Performance optimizations for high-throughput logging
      read_from_head false
      read_lines_limit 1000
      refresh_interval 1
    </source>
    
    # Filter for Bolt performance metrics extraction
    <filter bolt.app>
      @type grep
      <regexp>
        key level
        pattern ^(error|warn|info)$
      </regexp>
    </filter>
    
    # Add Kubernetes metadata
    <filter bolt.app>
      @type kubernetes_metadata
      @id kubernetes_metadata_bolt
      kubernetes_url "#{ENV['KUBERNETES_SERVICE_HOST']}:#{ENV['KUBERNETES_SERVICE_PORT_HTTPS']}"
      verify_ssl false
      ca_file /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      bearer_token_file /var/run/secrets/kubernetes.io/serviceaccount/token
      merge_json_log true
      preserve_json_log true
    </filter>
    
    # Add Bolt-specific metadata
    <filter bolt.app>
      @type record_transformer
      @id record_transformer_bolt
      <record>
        service_name bolt-logging
        service_version v2.0.0
        zero_allocation true
        performance_tier critical
      </record>
    </filter>
    
    # Performance metrics extraction
    <filter bolt.app>
      @type parser
      key_name message
      reserve_data true
      <parse>
        @type regexp
        expression /duration:(?<duration_ns>\d+)ns/
      </parse>
    </filter>
    
    # Output to Elasticsearch
    <match bolt.app>
      @type elasticsearch
      @id elasticsearch_bolt
      host "#{ENV['FLUENT_ELASTICSEARCH_HOST']}"
      port "#{ENV['FLUENT_ELASTICSEARCH_PORT']}"
      index_name "#{ENV['FLUENT_ELASTICSEARCH_INDEX_NAME']}"
      type_name "_doc"
      
      # Performance optimizations
      bulk_message_request_threshold 1024
      bulk_message_flush_interval 1s
      chunk_limit_size 10MB
      queue_limit_length 512
      retry_max_interval 10s
      
      # Template for Bolt-specific mappings
      template_name bolt_logs
      template_file /fluentd/etc/bolt_template.json
      
      <buffer>
        @type file
        path /tmp/fluentd-bolt-buffer
        flush_mode interval
        flush_interval 1s
        chunk_limit_size 10MB
        total_limit_size 1GB
        retry_type exponential_backoff
        retry_max_times 10
        retry_max_interval 30s
      </buffer>
    </match>
    
    # Health check endpoint
    <source>
      @type http
      bind 0.0.0.0
      port 8080
    </source>
  
  bolt_template.json: |
    {
      "index_patterns": ["bolt-logs-*"],
      "settings": {
        "number_of_shards": 1,
        "number_of_replicas": 1,
        "index.refresh_interval": "1s"
      },
      "mappings": {
        "properties": {
          "@timestamp": {
            "type": "date"
          },
          "level": {
            "type": "keyword"
          },
          "message": {
            "type": "text",
            "analyzer": "standard"
          },
          "duration_ns": {
            "type": "long"
          },
          "zero_allocation": {
            "type": "boolean"
          },
          "service_name": {
            "type": "keyword"
          },
          "service_version": {
            "type": "keyword"
          },
          "kubernetes": {
            "properties": {
              "namespace_name": {
                "type": "keyword"
              },
              "pod_name": {
                "type": "keyword"
              },
              "container_name": {
                "type": "keyword"
              },
              "node_name": {
                "type": "keyword"
              }
            }
          }
        }
      }
    }

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: falco-config
  namespace: bolt-logging
data:
  falco.yaml: |
    # Falco configuration for Bolt security monitoring
    
    # Output settings
    json_output: true
    json_include_output_property: true
    json_include_tags_property: true
    
    # Logging
    log_stderr: true
    log_syslog: false
    log_level: info
    
    # Performance settings for high-throughput environments
    buffered_outputs: true
    outputs_queue:
      capacity: 10000
      
    # Rate limiting to prevent overwhelming monitoring systems
    outputs:
      rate: 100
      max_burst: 1000
    
    # Rules configuration
    rules_file:
      - /etc/falco/falco_rules.yaml
      - /etc/falco/bolt_rules.yaml
    
    # Plugins
    plugins:
      - name: k8s
        library_path: libk8s.so
        init_config:
          k8s_api: "https://kubernetes.default.svc"
          k8s_api_cert: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          k8s_api_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
    
    # Load plugins
    load_plugins: [k8s]
    
    # Outputs
    syslog_output:
      enabled: false
    
    file_output:
      enabled: true
      keep_alive: false
      filename: /var/log/falco/events.log
    
    stdout_output:
      enabled: true
      
    http_output:
      enabled: true
      url: http://falco-exporter:9376/webhook
      user_agent: "falco/bolt-monitoring"
  
  bolt_rules.yaml: |
    # Bolt-specific Falco security rules
    
    - rule: Bolt Application Unexpected Network Connection
      desc: Detect unexpected network connections from Bolt application
      condition: >
        spawned_process and 
        container and 
        k8s_ns=bolt-logging and 
        k8s_pod_label_app=bolt-app and
        (outbound or inbound) and
        not fd.net.ip in (prometheus_ips, jaeger_ips, dns_ips)
      output: >
        Unexpected network connection from Bolt app
        (user=%user.name command=%proc.cmdline connection=%fd.name
        container=%container.name image=%container.image.repository)
      priority: WARNING
      tags: [network, bolt, security]
    
    - rule: Bolt Zero-Allocation Violation
      desc: Monitor for unexpected memory allocations in Bolt
      condition: >
        spawned_process and 
        container and 
        k8s_ns=bolt-logging and 
        k8s_pod_label_app=bolt-app and
        proc.name contains "bolt" and
        (proc.args contains "malloc" or proc.args contains "alloc")
      output: >
        Potential zero-allocation violation in Bolt
        (user=%user.name command=%proc.cmdline 
        container=%container.name)
      priority: HIGH
      tags: [performance, bolt, allocation]
    
    - rule: Bolt Configuration File Access
      desc: Monitor access to Bolt configuration files
      condition: >
        open_read and 
        container and 
        k8s_ns=bolt-logging and 
        k8s_pod_label_app=bolt-app and
        fd.name startswith "/config/"
      output: >
        Bolt configuration file accessed
        (user=%user.name file=%fd.name 
        container=%container.name command=%proc.cmdline)
      priority: INFO
      tags: [configuration, bolt, access]
    
    - rule: Bolt Performance Degradation
      desc: Detect potential performance issues in Bolt
      condition: >
        spawned_process and 
        container and 
        k8s_ns=bolt-logging and 
        k8s_pod_label_app=bolt-app and
        (proc.cpu_usage > 80 or proc.memory_usage > 90)
      output: >
        Bolt performance degradation detected
        (cpu_usage=%proc.cpu_usage memory_usage=%proc.memory_usage
        container=%container.name)
      priority: WARNING
      tags: [performance, bolt, resource]

---
# Custom Bolt Metrics Collector
apiVersion: v1
kind: ConfigMap
metadata:
  name: bolt-metrics-collector-config
  namespace: bolt-logging
data:
  config.yaml: |
    # Bolt Metrics Collector Configuration
    collector:
      name: bolt-metrics-collector
      version: "1.0.0"
      interval: 1s
      
    # Source configuration
    sources:
      - name: bolt-app
        type: prometheus
        endpoint: http://localhost:9090/metrics
        timeout: 500ms
        
    # Metrics processing rules
    processors:
      # Zero-allocation compliance monitoring
      - name: zero_allocation_checker
        type: threshold
        metric: bolt_allocation_count_total
        threshold: 0
        operator: greater_than
        alert: true
        severity: critical
        
      # Latency SLA monitoring  
      - name: latency_sla_checker
        type: percentile
        metric: bolt_logging_duration_seconds
        percentile: 95
        threshold: 0.0001  # 100Î¼s
        operator: greater_than
        alert: true
        severity: warning
        
      # Throughput monitoring
      - name: throughput_monitor
        type: rate
        metric: bolt_log_events_total
        window: 1m
        threshold: 100000  # 100k events/sec
        operator: greater_than
        alert: false
        
      # Error rate monitoring
      - name: error_rate_monitor
        type: ratio
        numerator: bolt_logging_errors_total
        denominator: bolt_log_events_total
        window: 5m
        threshold: 0.01  # 1%
        operator: greater_than
        alert: true
        severity: warning
        
    # Output configuration
    outputs:
      - name: prometheus
        type: prometheus
        listen: "0.0.0.0:9091"
        path: "/metrics"
        
      - name: alerts
        type: webhook
        url: http://alertmanager:9093/api/v1/alerts
        timeout: 5s
        
    # Performance settings
    performance:
      buffer_size: 10000
      batch_size: 1000
      flush_interval: 1s
      max_memory: 128MB
      
    # Logging
    logging:
      level: info
      format: json

---
# DaemonSet for Node-level monitoring
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: bolt-node-monitor
  namespace: bolt-logging
  labels:
    app: bolt-node-monitor
spec:
  selector:
    matchLabels:
      app: bolt-node-monitor
  template:
    metadata:
      labels:
        app: bolt-node-monitor
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9100"
    spec:
      hostNetwork: true
      hostPID: true
      hostIPC: true
      
      tolerations:
      - operator: Exists
        effect: NoSchedule
      - operator: Exists
        effect: NoExecute
      
      containers:
      # Enhanced Node Exporter with Bolt-specific collectors
      - name: node-exporter
        image: prom/node-exporter:v1.6.1
        args:
        - --path.procfs=/host/proc
        - --path.sysfs=/host/sys
        - --path.rootfs=/host/root
        - --collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)
        - --collector.cpu.info
        - --collector.meminfo
        - --collector.diskstats
        - --collector.netdev
        - --collector.loadavg
        - --collector.systemd
        - --collector.processes
        ports:
        - containerPort: 9100
          hostPort: 9100
          name: metrics
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 200m
            memory: 256Mi
        volumeMounts:
        - name: proc
          mountPath: /host/proc
          readOnly: true
        - name: sys
          mountPath: /host/sys
          readOnly: true
        - name: root
          mountPath: /host/root
          readOnly: true
          mountPropagation: HostToContainer
        securityContext:
          runAsNonRoot: true
          runAsUser: 65534
      
      # Custom Bolt system monitor
      - name: bolt-system-monitor
        image: bolt-logging/system-monitor:v1.0.0
        env:
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: MONITOR_INTERVAL
          value: "5s"
        ports:
        - containerPort: 9102
          hostPort: 9102
          name: bolt-metrics
        resources:
          requests:
            cpu: 50m
            memory: 64Mi
          limits:
            cpu: 100m
            memory: 128Mi
        volumeMounts:
        - name: proc
          mountPath: /host/proc
          readOnly: true
        - name: sys
          mountPath: /host/sys
          readOnly: true
        securityContext:
          runAsNonRoot: true
          runAsUser: 65534
      
      volumes:
      - name: proc
        hostPath:
          path: /proc
      - name: sys
        hostPath:
          path: /sys
      - name: root
        hostPath:
          path: /

---
# Service for DaemonSet monitoring
apiVersion: v1
kind: Service
metadata:
  name: bolt-node-monitor
  namespace: bolt-logging
  labels:
    app: bolt-node-monitor
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "9100"
spec:
  type: ClusterIP
  clusterIP: None
  ports:
  - name: node-metrics
    port: 9100
    targetPort: 9100
  - name: bolt-system-metrics
    port: 9102
    targetPort: 9102
  selector:
    app: bolt-node-monitor

---
# ServiceMonitor for DaemonSet
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: bolt-node-monitor
  namespace: bolt-logging
  labels:
    app: bolt-node-monitor
    prometheus: kube-prometheus
spec:
  selector:
    matchLabels:
      app: bolt-node-monitor
  endpoints:
  - port: node-metrics
    path: /metrics
    interval: 15s
    scrapeTimeout: 10s
  - port: bolt-system-metrics
    path: /metrics
    interval: 5s
    scrapeTimeout: 3s

---
# Prometheus Rule for Bolt-specific alerts
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: bolt-sidecar-alerts
  namespace: bolt-logging
  labels:
    prometheus: kube-prometheus
spec:
  groups:
  - name: bolt.sidecars
    interval: 10s
    rules:
    - alert: BoltFluentdDown
      expr: up{job="bolt-app-service", port="fluentd"} == 0
      for: 30s
      labels:
        severity: warning
        component: logging
      annotations:
        summary: "Bolt Fluentd sidecar is down"
        description: "Fluentd log shipping sidecar is not responding"
        
    - alert: BoltFalcoDown
      expr: up{job="bolt-app-service", port="falco"} == 0
      for: 30s
      labels:
        severity: warning
        component: security
      annotations:
        summary: "Bolt Falco sidecar is down"
        description: "Falco security monitoring sidecar is not responding"
        
    - alert: BoltNodeExporterDown
      expr: up{job="bolt-node-monitor", port="node-metrics"} == 0
      for: 1m
      labels:
        severity: critical
        component: monitoring
      annotations:
        summary: "Bolt Node Exporter is down"
        description: "Node-level metrics collection is not working on {{ $labels.instance }}"