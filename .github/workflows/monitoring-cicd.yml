# Enhanced CI/CD Pipeline for Bolt Monitoring Stack
# Performance regression detection, security scanning, and automated deployment

name: Bolt Monitoring CI/CD

on:
  push:
    branches: [ main, develop, 'release/*' ]
    paths:
      - 'monitoring/**'
      - 'infrastructure/**'
      - '.github/workflows/monitoring-cicd.yml'
  pull_request:
    branches: [ main, develop ]
    paths:
      - 'monitoring/**'
      - 'infrastructure/**'
  schedule:
    # Run security scans daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      environment:
        description: 'Target deployment environment'
        required: true
        default: 'staging'
        type: choice
        options:
        - staging
        - production
      skip_tests:
        description: 'Skip performance tests'
        required: false
        default: false
        type: boolean

env:
  # Global environment variables
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}
  BOLT_VERSION: v2.0.0
  NODE_VERSION: '18'
  GO_VERSION: '1.21'
  TERRAFORM_VERSION: '1.6.0'
  HELM_VERSION: '3.13.0'
  
  # Performance thresholds
  MAX_LATENCY_NS: 100000  # 100Œºs SLA
  MIN_THROUGHPUT_OPS: 10000000  # 10M ops/sec
  MAX_ALLOCATIONS: 0  # Zero-allocation requirement
  MAX_ERROR_RATE: 0.001  # 0.1% error rate

jobs:
  # Security scanning and validation
  security-scan:
    name: Security Analysis
    runs-on: ubuntu-latest
    permissions:
      contents: read
      security-events: write
      actions: read
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for security analysis
    
    - name: Set up Go
      uses: actions/setup-go@v4
      with:
        go-version: ${{ env.GO_VERSION }}
        cache: true
    
    - name: Cache Go modules
      uses: actions/cache@v3
      with:
        path: |
          ~/.cache/go-build
          ~/go/pkg/mod
        key: ${{ runner.os }}-go-${{ hashFiles('**/go.sum') }}
        restore-keys: |
          ${{ runner.os }}-go-
    
    # Secret scanning (skip on main branch push to avoid BASE==HEAD error)
    - name: Run secret scan
      if: github.event_name == 'pull_request' || github.ref != 'refs/heads/main'
      uses: trufflesecurity/trufflehog@main
      with:
        path: ./
        base: main
        head: HEAD
        extra_args: --debug --only-verified
    
    # For main branch pushes, scan the entire repository
    - name: Run secret scan (main branch)
      if: github.event_name == 'push' && github.ref == 'refs/heads/main'
      uses: trufflesecurity/trufflehog@main
      with:
        path: ./
        extra_args: --debug --only-verified
    
    # SAST scanning
    - name: Run Gosec security scanner
      uses: securego/gosec@master
      with:
        args: '-fmt sarif -out gosec-results.sarif ./...'
      continue-on-error: true
    
    - name: Upload Gosec SARIF file
      if: always() && hashFiles('gosec-results.sarif') != ''
      uses: github/codeql-action/upload-sarif@v3
      with:
        sarif_file: gosec-results.sarif
    
    # Container image scanning
    - name: Build temporary image for scanning
      run: |
        docker build -t bolt-temp:${{ github.sha }} -f examples/cloud-native/docker-compose/app/Dockerfile .
    
    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        image-ref: 'bolt-temp:${{ github.sha }}'
        format: 'sarif'
        output: 'trivy-results.sarif'
    
    - name: Upload Trivy SARIF file
      if: always() && hashFiles('trivy-results.sarif') != ''
      uses: github/codeql-action/upload-sarif@v3
      with:
        sarif_file: 'trivy-results.sarif'
    
    # Infrastructure security scanning
    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v2
      with:
        terraform_version: ${{ env.TERRAFORM_VERSION }}
    
    - name: Run Checkov IaC security scan
      uses: bridgecrewio/checkov-action@master
      with:
        directory: infrastructure/terraform
        framework: terraform
        output_format: sarif
        output_file_path: checkov-results.sarif
      continue-on-error: true
    
    - name: Upload Checkov SARIF file
      if: always() && hashFiles('checkov-results.sarif') != ''
      uses: github/codeql-action/upload-sarif@v3
      with:
        sarif_file: checkov-results.sarif
    
    # Kubernetes security scanning
    - name: Run Kubesec security scan
      run: |
        curl -sSX POST --data-binary @infrastructure/kubernetes/bolt-app-deployment.yaml \
          https://v2.kubesec.io/scan > kubesec-results.json
        cat kubesec-results.json | jq .
        # Check if score is acceptable (> 0)
        SCORE=$(cat kubesec-results.json | jq '.[0].score')
        if [ "$SCORE" -lt 0 ]; then
          echo "Kubesec security score is below threshold: $SCORE"
          exit 1
        fi
    
    # Helm chart security validation
    - name: Setup Helm
      uses: azure/setup-helm@v3
      with:
        version: ${{ env.HELM_VERSION }}
    
    - name: Validate Helm charts
      run: |
        cd infrastructure/helm/bolt-monitoring
        helm lint .
        helm template . --dry-run > /tmp/helm-output.yaml
        # Run Polaris security scan on Helm output
        docker run --rm -v /tmp:/tmp fairwindsops/polaris:latest \
          polaris audit /tmp/helm-output.yaml --format=json > polaris-results.json
        cat polaris-results.json | jq .

  # Configuration validation and testing
  config-validation:
    name: Configuration Validation
    runs-on: ubuntu-latest
    needs: [security-scan]
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Setup Prometheus
      run: |
        wget https://github.com/prometheus/prometheus/releases/download/v2.47.0/prometheus-2.47.0.linux-amd64.tar.gz
        tar xvfz prometheus-2.47.0.linux-amd64.tar.gz
        sudo mv prometheus-2.47.0.linux-amd64/promtool /usr/local/bin/
    
    - name: Validate Prometheus configuration
      run: |
        promtool check config monitoring/prometheus/prometheus.yml
        promtool check rules monitoring/prometheus/rules/bolt-recording.yml
        promtool check rules monitoring/prometheus/alerts/bolt-performance.yml
    
    - name: Setup AlertManager
      run: |
        wget https://github.com/prometheus/alertmanager/releases/download/v0.26.0/alertmanager-0.26.0.linux-amd64.tar.gz
        tar xvfz alertmanager-0.26.0.linux-amd64.tar.gz
        sudo mv alertmanager-0.26.0.linux-amd64/amtool /usr/local/bin/
    
    - name: Validate AlertManager configuration
      run: |
        amtool check-config monitoring/alertmanager/alertmanager.yml
    
    - name: Validate Docker Compose
      run: |
        cd monitoring/docker-compose
        docker-compose config
        docker-compose -f docker-compose.yml config
    
    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v2
      with:
        terraform_version: ${{ env.TERRAFORM_VERSION }}
    
    - name: Validate Terraform configuration
      run: |
        cd infrastructure/terraform
        terraform init -backend=false
        terraform validate
        terraform fmt -check=true
    
    - name: Setup Helm
      uses: azure/setup-helm@v3
      with:
        version: ${{ env.HELM_VERSION }}
    
    - name: Validate Helm charts
      run: |
        cd infrastructure/helm/bolt-monitoring
        helm dependency update
        helm lint .
        helm template . --dry-run > /dev/null

  # Performance regression testing
  performance-testing:
    name: Performance Regression Tests
    runs-on: ubuntu-latest
    if: ${{ !inputs.skip_tests }}
    needs: [config-validation]
    
    strategy:
      matrix:
        test-type: ['latency', 'throughput', 'memory', 'concurrent']
        go-version: ['1.21', '1.22']
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Go
      uses: actions/setup-go@v4
      with:
        go-version: ${{ matrix.go-version }}
        cache: true
    
    - name: Cache Go modules
      uses: actions/cache@v3
      with:
        path: |
          ~/.cache/go-build
          ~/go/pkg/mod
        key: ${{ runner.os }}-go-${{ matrix.go-version }}-${{ hashFiles('**/go.sum') }}
        restore-keys: |
          ${{ runner.os }}-go-${{ matrix.go-version }}-
    
    - name: Install dependencies
      run: |
        go mod download
        go install golang.org/x/perf/cmd/benchstat@latest
    
    # Baseline performance benchmarks
    - name: Run baseline benchmarks
      run: |
        cd ${{ github.workspace }}
        go test -bench=BenchmarkZeroAllocation -benchmem -count=10 -cpu=1,2,4 \
          -benchtime=10s > baseline-${{ matrix.test-type }}-${{ matrix.go-version }}.txt
        cat baseline-${{ matrix.test-type }}-${{ matrix.go-version }}.txt
    
    - name: Performance regression detection
      run: |
        cd ${{ github.workspace }}
        
        # Extract key metrics from benchmark
        LATENCY=$(grep "BenchmarkZeroAllocation" baseline-${{ matrix.test-type }}-${{ matrix.go-version }}.txt | \
          tail -1 | awk '{print $3}' | sed 's/ns\/op//')
        ALLOCATIONS=$(grep "BenchmarkZeroAllocation" baseline-${{ matrix.test-type }}-${{ matrix.go-version }}.txt | \
          tail -1 | awk '{print $5}' | sed 's/allocs\/op//')
        THROUGHPUT=$(echo "scale=0; 1000000000 / $LATENCY" | bc)
        
        echo "Performance Metrics:"
        echo "Latency: ${LATENCY}ns/op"
        echo "Allocations: ${ALLOCATIONS}allocs/op"
        echo "Throughput: ${THROUGHPUT}ops/sec"
        
        # Performance validation
        if (( $(echo "$LATENCY > ${{ env.MAX_LATENCY_NS }}" | bc -l) )); then
          echo "‚ùå PERFORMANCE REGRESSION: Latency ${LATENCY}ns exceeds SLA of ${{ env.MAX_LATENCY_NS }}ns"
          exit 1
        fi
        
        if (( $(echo "$ALLOCATIONS > ${{ env.MAX_ALLOCATIONS }}" | bc -l) )); then
          echo "‚ùå ZERO-ALLOCATION VIOLATION: ${ALLOCATIONS} allocations detected"
          exit 1
        fi
        
        if (( $(echo "$THROUGHPUT < ${{ env.MIN_THROUGHPUT_OPS }}" | bc -l) )); then
          echo "‚ùå THROUGHPUT REGRESSION: ${THROUGHPUT}ops/sec below minimum ${{ env.MIN_THROUGHPUT_OPS }}"
          exit 1
        fi
        
        echo "‚úÖ All performance metrics within acceptable ranges"
        
        # Store metrics for comparison
        echo "$LATENCY,$ALLOCATIONS,$THROUGHPUT" > performance-metrics-${{ matrix.test-type }}-${{ matrix.go-version }}.csv
    
    - name: Memory profiling
      if: matrix.test-type == 'memory'
      run: |
        cd ${{ github.workspace }}
        go test -bench=BenchmarkZeroAllocation -memprofile=mem-${{ matrix.go-version }}.prof -benchmem
        go tool pprof -top mem-${{ matrix.go-version }}.prof > memory-profile-${{ matrix.go-version }}.txt
        
        # Check for unexpected allocations
        HEAP_ALLOCS=$(grep -o 'flat.*alloc_space' memory-profile-${{ matrix.go-version }}.txt | head -1 | awk '{print $1}' | sed 's/[^0-9.]//g')
        if (( $(echo "$HEAP_ALLOCS > 0" | bc -l) )); then
          echo "‚ö†Ô∏è  Warning: Heap allocations detected: ${HEAP_ALLOCS}"
          cat memory-profile-${{ matrix.go-version }}.txt
        fi
    
    - name: CPU profiling
      if: matrix.test-type == 'latency'
      run: |
        cd ${{ github.workspace }}
        go test -bench=BenchmarkZeroAllocation -cpuprofile=cpu-${{ matrix.go-version }}.prof
        go tool pprof -top cpu-${{ matrix.go-version }}.prof > cpu-profile-${{ matrix.go-version }}.txt
        cat cpu-profile-${{ matrix.go-version }}.txt
    
    - name: Race condition testing
      if: matrix.test-type == 'concurrent'
      run: |
        cd ${{ github.workspace }}
        go test -race -short -run=. -bench=. -benchtime=5s
    
    - name: Upload performance artifacts
      uses: actions/upload-artifact@v3
      with:
        name: performance-results-${{ matrix.test-type }}-${{ matrix.go-version }}
        path: |
          baseline-*.txt
          performance-metrics-*.csv
          *-profile-*.txt
          *.prof
        retention-days: 30
    
    - name: Performance comparison
      if: github.event_name == 'pull_request'
      run: |
        # Download baseline from main branch for comparison
        # This would typically fetch from a performance database or artifact store
        echo "üìä Performance comparison against main branch:"
        echo "Current: $(cat performance-metrics-${{ matrix.test-type }}-${{ matrix.go-version }}.csv)"
        # TODO: Implement historical comparison logic

  # Load and stress testing
  load-testing:
    name: Load & Stress Testing
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main' || github.event_name == 'workflow_dispatch'
    needs: [performance-testing]
    
    services:
      prometheus:
        image: prom/prometheus:v2.47.0
        ports:
          - 9090:9090
        options: >-
          --health-cmd "wget --no-verbose --tries=1 --spider http://localhost:9090/-/healthy || exit 1"
          --health-interval 30s
          --health-timeout 10s
          --health-retries 3
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Go
      uses: actions/setup-go@v4
      with:
        go-version: ${{ env.GO_VERSION }}
    
    - name: Build Bolt application
      run: |
        cd examples/cloud-native/docker-compose/app
        go mod download
        go build -o bolt-app .
        chmod +x bolt-app
    
    - name: Start Bolt application
      run: |
        cd examples/cloud-native/docker-compose/app
        ./bolt-app &
        APP_PID=$!
        echo "APP_PID=$APP_PID" >> $GITHUB_ENV
        
        # Wait for app to start
        for i in {1..30}; do
          if curl -f http://localhost:8080/health; then
            break
          fi
          sleep 1
        done
    
    - name: Setup k6 load testing
      run: |
        sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys C5AD17C747E3415A3642D57D77C6C491D6AC1D69
        echo "deb https://dl.k6.io/deb stable main" | sudo tee /etc/apt/sources.list.d/k6.list
        sudo apt-get update
        sudo apt-get install k6
    
    - name: Run load test
      run: |
        cat > load-test.js << 'EOF'
        import http from 'k6/http';
        import { check, sleep } from 'k6';
        
        export let options = {
          stages: [
            { duration: '1m', target: 100 },   // Ramp up
            { duration: '3m', target: 100 },   // Steady state
            { duration: '1m', target: 500 },   // Spike test
            { duration: '2m', target: 500 },   // Sustained load
            { duration: '1m', target: 0 },     // Ramp down
          ],
          thresholds: {
            http_req_duration: ['p(95)<100'],  // 95% under 100ms
            http_req_rate: ['rate>1000'],      // > 1000 req/s
            http_req_failed: ['rate<0.01'],    // < 1% errors
          },
        };
        
        export default function () {
          // Test logging endpoint with varying payloads
          let response = http.post('http://localhost:8080/log', JSON.stringify({
            level: 'info',
            message: 'Load test message',
            timestamp: new Date().toISOString(),
            user_id: Math.floor(Math.random() * 10000),
            request_id: __VU + '-' + __ITER,
            data: {
              action: 'load_test',
              iteration: __ITER,
              virtual_user: __VU,
            }
          }), {
            headers: { 'Content-Type': 'application/json' },
          });
          
          check(response, {
            'status is 200': (r) => r.status === 200,
            'response time < 100ms': (r) => r.timings.duration < 100,
          });
          
          // Check metrics endpoint
          if (__ITER % 100 === 0) {
            let metricsResponse = http.get('http://localhost:8080/metrics');
            check(metricsResponse, {
              'metrics endpoint available': (r) => r.status === 200,
            });
          }
          
          sleep(0.1);  // 100ms think time
        }
        EOF
        
        k6 run load-test.js --out json=load-test-results.json
    
    - name: Analyze load test results
      run: |
        if [ -f load-test-results.json ]; then
          # Extract key metrics
          AVG_RESPONSE_TIME=$(cat load-test-results.json | jq -r 'select(.metric=="http_req_duration" and .type=="Point") | .data.value' | awk '{sum+=$1; count++} END {print sum/count}')
          ERROR_RATE=$(cat load-test-results.json | jq -r 'select(.metric=="http_req_failed" and .type=="Point") | .data.value' | awk '{sum+=$1; count++} END {print sum/count}')
          
          echo "Load Test Results:"
          echo "Average Response Time: ${AVG_RESPONSE_TIME}ms"
          echo "Error Rate: ${ERROR_RATE}%"
          
          # Validate against thresholds
          if (( $(echo "$AVG_RESPONSE_TIME > 100" | bc -l) )); then
            echo "‚ùå Load test failed: Average response time exceeds 100ms"
            exit 1
          fi
          
          if (( $(echo "$ERROR_RATE > 1" | bc -l) )); then
            echo "‚ùå Load test failed: Error rate exceeds 1%"
            exit 1
          fi
          
          echo "‚úÖ Load test passed all thresholds"
        fi
    
    - name: Stress test
      run: |
        cat > stress-test.js << 'EOF'
        import http from 'k6/http';
        import { check } from 'k6';
        
        export let options = {
          stages: [
            { duration: '30s', target: 1000 },   // Quick ramp to high load
            { duration: '1m', target: 2000 },    // Increase to breaking point
            { duration: '30s', target: 0 },      // Quick ramp down
          ],
          thresholds: {
            http_req_failed: ['rate<0.1'],       // Allow higher error rate in stress test
          },
        };
        
        export default function () {
          let response = http.post('http://localhost:8080/log', JSON.stringify({
            level: 'info',
            message: 'Stress test message ' + __VU + '-' + __ITER,
            timestamp: new Date().toISOString(),
          }), {
            headers: { 'Content-Type': 'application/json' },
          });
          
          check(response, {
            'status is not 500': (r) => r.status !== 500,  // System should handle gracefully
          });
        }
        EOF
        
        k6 run stress-test.js --out json=stress-test-results.json || true  # Don't fail on stress test
    
    - name: Cleanup
      if: always()
      run: |
        if [ -n "$APP_PID" ]; then
          kill $APP_PID || true
        fi
    
    - name: Upload load test artifacts
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: load-test-results
        path: |
          load-test-results.json
          stress-test-results.json
          load-test.js
          stress-test.js
        retention-days: 30

  # Build and push container images
  build-images:
    name: Build & Push Images
    runs-on: ubuntu-latest
    needs: [security-scan, config-validation]
    if: github.event_name == 'push' || github.event_name == 'workflow_dispatch'
    
    permissions:
      contents: read
      packages: write
    
    outputs:
      image-tags: ${{ steps.meta.outputs.tags }}
      image-digest: ${{ steps.build.outputs.digest }}
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Log in to Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=sha,prefix={{branch}}-
          type=raw,value=${{ env.BOLT_VERSION }}
          type=raw,value=latest,enable={{is_default_branch}}
    
    - name: Build and push Docker image
      id: build
      uses: docker/build-push-action@v5
      with:
        context: examples/cloud-native/docker-compose/app
        platforms: linux/amd64,linux/arm64
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max
        build-args: |
          BOLT_VERSION=${{ env.BOLT_VERSION }}
          BUILD_DATE=${{ steps.meta.outputs.labels.build-date }}
          VCS_REF=${{ github.sha }}
    
    # Sign container image with cosign
    - name: Install cosign
      uses: sigstore/cosign-installer@v3
    
    - name: Sign container image
      run: |
        cosign sign --yes ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}@${{ steps.build.outputs.digest }}
      env:
        COSIGN_EXPERIMENTAL: 1

  # Deploy to staging
  deploy-staging:
    name: Deploy to Staging
    runs-on: ubuntu-latest
    needs: [performance-testing, build-images]
    if: github.ref == 'refs/heads/develop' || (github.event_name == 'workflow_dispatch' && inputs.environment == 'staging')
    
    environment:
      name: staging
      url: https://staging.bolt-monitoring.local
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Setup Helm
      uses: azure/setup-helm@v3
      with:
        version: ${{ env.HELM_VERSION }}
    
    - name: Configure kubectl
      uses: azure/k8s-set-context@v3
      with:
        method: kubeconfig
        kubeconfig: ${{ secrets.STAGING_KUBECONFIG }}
    
    - name: Deploy to staging
      run: |
        cd infrastructure/helm/bolt-monitoring
        
        # Update dependencies
        helm dependency update
        
        # Deploy with staging values
        helm upgrade --install bolt-monitoring . \
          --namespace bolt-staging \
          --create-namespace \
          --values values.yaml \
          --values values-staging.yaml \
          --set boltApp.image.tag=${{ needs.build-images.outputs.image-digest }} \
          --set global.environment=staging \
          --wait --timeout=10m
    
    - name: Run smoke tests
      run: |
        kubectl wait --for=condition=available --timeout=300s deployment/bolt-app -n bolt-staging
        
        # Test application health
        kubectl run --rm -it staging-test --image=curlimages/curl:8.4.0 --restart=Never -- \
          curl -f http://bolt-app.bolt-staging.svc.cluster.local/health
        
        # Test metrics endpoint
        kubectl run --rm -it staging-metrics-test --image=curlimages/curl:8.4.0 --restart=Never -- \
          curl -f http://bolt-app.bolt-staging.svc.cluster.local:9090/metrics

  # Deploy to production
  deploy-production:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: [performance-testing, build-images, deploy-staging]
    if: github.ref == 'refs/heads/main' || (github.event_name == 'workflow_dispatch' && inputs.environment == 'production')
    
    environment:
      name: production
      url: https://bolt-monitoring.local
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Setup Helm
      uses: azure/setup-helm@v3
      with:
        version: ${{ env.HELM_VERSION }}
    
    - name: Configure kubectl
      uses: azure/k8s-set-context@v3
      with:
        method: kubeconfig
        kubeconfig: ${{ secrets.PRODUCTION_KUBECONFIG }}
    
    - name: Pre-deployment checks
      run: |
        # Verify cluster health
        kubectl get nodes
        kubectl top nodes
        
        # Check current deployment status
        helm list -n bolt-production || echo "No existing deployment"
    
    - name: Deploy to production
      run: |
        cd infrastructure/helm/bolt-monitoring
        
        # Update dependencies
        helm dependency update
        
        # Production deployment with blue-green strategy
        helm upgrade --install bolt-monitoring . \
          --namespace bolt-production \
          --create-namespace \
          --values values.yaml \
          --values values-production.yaml \
          --set boltApp.image.tag=${{ needs.build-images.outputs.image-digest }} \
          --set global.environment=production \
          --wait --timeout=15m
    
    - name: Post-deployment validation
      run: |
        # Wait for all pods to be ready
        kubectl wait --for=condition=available --timeout=600s deployment/bolt-app -n bolt-production
        
        # Run comprehensive health checks
        kubectl run --rm -it prod-test --image=curlimages/curl:8.4.0 --restart=Never -- \
          sh -c "
          curl -f http://bolt-app.bolt-production.svc.cluster.local/health &&
          curl -f http://prometheus.bolt-production.svc.cluster.local:9090/-/healthy &&
          curl -f http://grafana.bolt-production.svc.cluster.local:3000/api/health
          "
        
        # Performance validation
        kubectl run --rm -it prod-perf-test --image=bolt-logging/performance-tester:latest --restart=Never -- \
          bolt-perf-test --target http://bolt-app.bolt-production.svc.cluster.local --duration 30s --sla 100000
    
    - name: Update deployment status
      run: |
        # Create deployment annotation
        kubectl annotate deployment/bolt-app -n bolt-production \
          deployment.kubernetes.io/revision-history="$(date -u +%Y%m%d%H%M%S)-${{ github.sha }}"

  # Post-deployment monitoring
  post-deployment:
    name: Post-Deployment Monitoring
    runs-on: ubuntu-latest
    needs: [deploy-production]
    if: always() && (needs.deploy-production.result == 'success')
    
    steps:
    - name: Monitor deployment
      run: |
        echo "üöÄ Deployment completed successfully!"
        echo "üìä Monitoring production metrics for the next 5 minutes..."
        
        # This would typically integrate with monitoring tools
        sleep 300  # Monitor for 5 minutes
        
        echo "‚úÖ Deployment monitoring completed"
    
    - name: Notify teams
      uses: 8398a7/action-slack@v3
      with:
        status: custom
        custom_payload: |
          {
            text: "üöÄ Bolt Monitoring Stack deployed to production",
            attachments: [{
              color: 'good',
              fields: [{
                title: 'Environment',
                value: 'Production',
                short: true
              }, {
                title: 'Version',
                value: '${{ env.BOLT_VERSION }}',
                short: true
              }, {
                title: 'Commit',
                value: '${{ github.sha }}',
                short: true
              }, {
                title: 'Performance',
                value: 'Sub-100Œºs latency maintained',
                short: true
              }]
            }]
          }
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
      if: success()

# Cleanup job for failed deployments
  cleanup-on-failure:
    name: Cleanup Failed Deployment
    runs-on: ubuntu-latest
    needs: [deploy-staging, deploy-production]
    if: failure()
    
    steps:
    - name: Rollback on failure
      run: |
        echo "üîÑ Rolling back failed deployment..."
        # This would implement rollback logic
        echo "‚úÖ Rollback completed"
    
    - name: Notify failure
      if: env.SLACK_WEBHOOK_URL != ''
      uses: 8398a7/action-slack@v3
      with:
        status: failure
        text: "‚ùå Bolt Monitoring Stack deployment failed"
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}